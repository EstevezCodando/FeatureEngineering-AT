{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55239390",
   "metadata": {},
   "source": [
    "## Questão A — Definição de *feature* e necessidade de tratamento\n",
    "\n",
    "**Feature** é um atributo coletado, uma variável preditora que representa um aspecto observado de um fenômeno-alvo, o alvo por outro lado é chamado \"target\". Em termos formais, cada feature é um componente de um vetor que alimenta o algoritmo de Machine Learning.\n",
    "\n",
    "### Por que tratar features antes da modelagem?\n",
    "\n",
    "1. **Escalas incompatíveis** – Distâncias e gradientes ficam distorcidos; resolve-se com normalização ou padronização.  \n",
    "2. **Tipos incompatíveis** – Algoritmos numéricos não processam categorias brutas; emprega‐se *encoding* para dados qualitativos.  \n",
    "3. **Dados ausentes e ruído** – Valores nulos (*NaN*) e outliers prejudicam a convergência; usa-se imputação e *robust scaling*.  \n",
    "4. **Redundância e vazamento** – Variáveis colineares ou com informação futura causam *overfitting*; eliminam-se via análise de correlação ou pipelines bem definidos.\n",
    "\n",
    "---\n",
    "\n",
    "## Questão B — Seleção de features úteis\n",
    "\n",
    "| Método      | Mecanismo                                                         | Quando usar                                  | Limitações                                                         |\n",
    "|-------------|-------------------------------------------------------------------|----------------------------------------------|--------------------------------------------------------------------|\n",
    "| **Filtragem** | Avalia cada feature isoladamente por estatísticas univariadas (teste qui-quadrado, ANOVA-F, informação mútua). | Datasets muito grandes; pré-poda rápida.      | Ignora interações e pode manter redundância.                       |\n",
    "| **Wrapper**   | Testa subconjuntos treinando um modelo (RFE, *forward/backward selection*). | Conjuntos médios; busca desempenho máximo para um algoritmo específico. | Alto custo computacional; risco de *overfitting* se validação fraca. |\n",
    "| **Embedding** | Seleção ocorre dentro do treinamento (Lasso, Árvores, mecanismos de atenção). | Dados de alta dimensão; modelos que já calculam importâncias. | Transparência reduzida; dependente do modelo escolhido.            |\n",
    "\n",
    "**Estratégia recomendada**  \n",
    "1. Aplicar filtragem para descartar irrelevâncias óbvias.  \n",
    "2. Utilizar wrapper ou embedding para refinar de acordo com o algoritmo-alvo.  \n",
    "3. Empregar validação cruzada para confirmar ganho de generalização.\n",
    "\n",
    "---\n",
    "\n",
    "## Questão C — Escalares, vetores e espaços vetoriais em ML\n",
    "\n",
    "| Conceito            | Definição resumida                                                                                   | Exemplo em Machine Learning                                                    |\n",
    "|---------------------|------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------|\n",
    "| **Escalar**         | Número real utilizado para ponderar vetores.                                                         | Taxa de aprendizagem (eta = 0,01) em *gradient descent*.                       |\n",
    "| **Vetor**           | Sequência ordenada de escalares, representada como x = (x₁, …, xₙ).                                   | Vetor de 784 pixels (28 × 28) que descreve uma imagem no conjunto MNIST.       |\n",
    "| **Espaço vetorial** | Conjunto V de vetores fechado sob adição e multiplicação por escalares, obedecendo a axiomas lineares.| O espaço R³⁰⁰ que contém todos os *word embeddings* de 300 dimensões (Word2Vec).|\n",
    "\n",
    "Esses conceitos sustentam operações essenciais em ML, como cálculo de distâncias (distância Euclidiana entre vetores), projeções e otimização em espaços de alta dimensão.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "508094c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.preprocessing import FunctionTransformer, PowerTransformer, MinMaxScaler, StandardScaler, Normalizer\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9fcd3ea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2(a)] Identificação das features contínuas\n",
      "\n",
      "Dataset contém 569 amostras e 30 features contínuas.\n",
      "\n",
      "\n",
      "[2(a)] Features contínuas no dataset:\n",
      " • mean radius\n",
      " • mean texture\n",
      " • mean perimeter\n",
      " • mean area\n",
      " • mean smoothness\n",
      " • mean compactness\n",
      " • mean concavity\n",
      " • mean concave points\n",
      " • mean symmetry\n",
      " • mean fractal dimension\n",
      " • radius error\n",
      " • texture error\n",
      " • perimeter error\n",
      " • area error\n",
      " • smoothness error\n",
      " • compactness error\n",
      " • concavity error\n",
      " • concave points error\n",
      " • symmetry error\n",
      " • fractal dimension error\n",
      " • worst radius\n",
      " • worst texture\n",
      " • worst perimeter\n",
      " • worst area\n",
      " • worst smoothness\n",
      " • worst compactness\n",
      " • worst concavity\n",
      " • worst concave points\n",
      " • worst symmetry\n",
      " • worst fractal dimension\n",
      "\n",
      "[2(b)] Discretização por bins fixos (4 intervalos de mesma largura)\n",
      "\n",
      "--- mean radius ---\n",
      "mean radius\n",
      "(6.959, 12.263]     191\n",
      "(12.263, 17.545]    276\n",
      "(17.545, 22.828]     90\n",
      "(22.828, 28.11]      12\n",
      "Name: count, dtype: int64 \n",
      "\n",
      "--- mean texture ---\n",
      "mean texture\n",
      "(9.679, 17.102]     185\n",
      "(17.102, 24.495]    316\n",
      "(24.495, 31.888]     64\n",
      "(31.888, 39.28]       4\n",
      "Name: count, dtype: int64 \n",
      "\n",
      "[2(c)] Discretização por bins variáveis (quartis)\n",
      "\n",
      "--- mean radius ---\n",
      "mean radius\n",
      "(6.9799999999999995, 11.7]    143\n",
      "(11.7, 13.37]                 142\n",
      "(13.37, 15.78]                142\n",
      "(15.78, 28.11]                142\n",
      "Name: count, dtype: int64 \n",
      "\n",
      "--- mean texture ---\n",
      "mean texture\n",
      "(9.709000000000001, 16.17]    143\n",
      "(16.17, 18.84]                142\n",
      "(18.84, 21.8]                 142\n",
      "(21.8, 39.28]                 142\n",
      "Name: count, dtype: int64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------------\n",
    "# Questão 2 — Breast Cancer Dataset\n",
    "# ---------------------------------------------------------------\n",
    "# Bibliotecas\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# Questão 2(a) — Identificar as features contínuas\n",
    "# ---------------------------------------------------------------\n",
    "dados = load_breast_cancer()\n",
    "df = pd.DataFrame(dados.data, columns=dados.feature_names)\n",
    "\n",
    "print(\"[2(a)] Identificação das features contínuas\\n\")\n",
    "print(f\"Dataset contém {df.shape[0]} amostras e {df.shape[1]} features contínuas.\\n\")\n",
    "\n",
    "# Todas as 30 colunas deste dataset já são numéricas contínuas.\n",
    "features_continuas = df.columns.tolist()\n",
    "print(\"\\n[2(a)] Features contínuas no dataset:\")\n",
    "for feat in features_continuas:\n",
    "    print(f\" • {feat}\")\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# Questão 2(b) — Discretização por bins fixos\n",
    "# ---------------------------------------------------------------\n",
    "# Selecionar duas features contínuas para o exemplo\n",
    "features_selecionadas = [\"mean radius\", \"mean texture\"]\n",
    "print(\"\\n[2(b)] Discretização por bins fixos (4 intervalos de mesma largura)\\n\")\n",
    "\n",
    "for coluna in features_selecionadas:\n",
    "    # 4 intervalos de igual largura\n",
    "    bins_fixos = pd.cut(df[coluna], bins=4, include_lowest=True)\n",
    "    contagem_fixos = bins_fixos.value_counts(sort=False)\n",
    "\n",
    "    print(f\"--- {coluna} ---\")\n",
    "    print(contagem_fixos, \"\\n\")\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# Questão 2(c) — Discretização por bins variáveis (quantis)\n",
    "# ---------------------------------------------------------------\n",
    "print(\"[2(c)] Discretização por bins variáveis (quartis)\\n\")\n",
    "\n",
    "for coluna in features_selecionadas:\n",
    "    # 4 intervalos gerados pelos quartis (mesmo nº aproximado de amostras)\n",
    "    bins_variaveis = pd.qcut(df[coluna], q=4, duplicates=\"drop\")\n",
    "    contagem_variaveis = bins_variaveis.value_counts(sort=False)\n",
    "\n",
    "    print(f\"--- {coluna} ---\")\n",
    "    print(contagem_variaveis, \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a702aeba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[2.A] Estatísticas após Min-Max Scaling\n",
      "                         min  max   mean    std\n",
      "mean radius              0.0  1.0  0.338  0.167\n",
      "mean texture             0.0  1.0  0.324  0.145\n",
      "mean perimeter           0.0  1.0  0.333  0.168\n",
      "mean area                0.0  1.0  0.217  0.149\n",
      "mean smoothness          0.0  1.0  0.395  0.127\n",
      "mean compactness         0.0  1.0  0.261  0.162\n",
      "mean concavity           0.0  1.0  0.208  0.187\n",
      "mean concave points      0.0  1.0  0.243  0.193\n",
      "mean symmetry            0.0  1.0  0.380  0.138\n",
      "mean fractal dimension   0.0  1.0  0.270  0.149\n",
      "radius error             0.0  1.0  0.106  0.100\n",
      "texture error            0.0  1.0  0.189  0.122\n",
      "perimeter error          0.0  1.0  0.099  0.095\n",
      "area error               0.0  1.0  0.063  0.085\n",
      "smoothness error         0.0  1.0  0.181  0.102\n",
      "compactness error        0.0  1.0  0.174  0.134\n",
      "concavity error          0.0  1.0  0.081  0.076\n",
      "concave points error     0.0  1.0  0.223  0.117\n",
      "symmetry error           0.0  1.0  0.178  0.116\n",
      "fractal dimension error  0.0  1.0  0.100  0.091\n",
      "worst radius             0.0  1.0  0.297  0.172\n",
      "worst texture            0.0  1.0  0.364  0.164\n",
      "worst perimeter          0.0  1.0  0.283  0.167\n",
      "worst area               0.0  1.0  0.171  0.140\n",
      "worst smoothness         0.0  1.0  0.404  0.151\n",
      "worst compactness        0.0  1.0  0.220  0.153\n",
      "worst concavity          0.0  1.0  0.217  0.166\n",
      "worst concave points     0.0  1.0  0.394  0.226\n",
      "worst symmetry           0.0  1.0  0.263  0.122\n",
      "worst fractal dimension  0.0  1.0  0.190  0.118\n",
      "\n",
      "[3.B] Estatísticas após PowerTransform (Yeo-Johnson)\n",
      "                         mean  std  skewness\n",
      "mean radius              -0.0  1.0     0.008\n",
      "mean texture              0.0  1.0     0.000\n",
      "mean perimeter            0.0  1.0     0.007\n",
      "mean area                 0.0  1.0     0.005\n",
      "mean smoothness           0.0  1.0    -0.010\n",
      "mean compactness          0.0  1.0     0.106\n",
      "mean concavity           -0.0  1.0     0.224\n",
      "mean concave points       0.0  1.0     0.199\n",
      "mean symmetry             0.0  1.0    -0.017\n",
      "mean fractal dimension   -0.0  1.0     0.036\n",
      "radius error              0.0  1.0     0.195\n",
      "texture error             0.0  1.0     0.013\n",
      "perimeter error           0.0  1.0     0.081\n",
      "area error               -0.0  1.0     0.069\n",
      "smoothness error          0.0  1.0     0.084\n",
      "compactness error         0.0  1.0     0.205\n",
      "concavity error           0.0  1.0     0.146\n",
      "concave points error      0.0  1.0    -0.038\n",
      "symmetry error            0.0  1.0     0.136\n",
      "fractal dimension error   0.0  1.0     0.235\n",
      "worst radius             -0.0  1.0     0.034\n",
      "worst texture             0.0  1.0    -0.004\n",
      "worst perimeter           0.0  1.0     0.031\n",
      "worst area                0.0  1.0     0.027\n",
      "worst smoothness          0.0  1.0    -0.001\n",
      "worst compactness         0.0  1.0     0.092\n",
      "worst concavity          -0.0  1.0     0.085\n",
      "worst concave points      0.0  1.0     0.050\n",
      "worst symmetry           -0.0  1.0    -0.053\n",
      "worst fractal dimension   0.0  1.0     0.082\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------------\n",
    "#Questão 3 Breast Cancer Dataset — Normalização\n",
    "# ---------------------------------------------------------------\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 3.A) Normalização personalizada via FunctionTransformer\n",
    "#    (Min-Max Scaling 0-1)\n",
    "# ---------------------------------------------------------------\n",
    "def min_max_scaler(arr):\n",
    "    \"\"\"Aplica min-max scaling coluna a coluna.\"\"\"\n",
    "    mins = np.min(arr, axis=0)\n",
    "    maxs = np.max(arr, axis=0)\n",
    "    return (arr - mins) / (maxs - mins)\n",
    "\n",
    "min_max_transformer = FunctionTransformer(min_max_scaler, feature_names_out=None)\n",
    "X_minmax = pd.DataFrame(\n",
    "    min_max_transformer.fit_transform(X.values),\n",
    "    columns=data.feature_names,\n",
    ")\n",
    "\n",
    "# Exibir estatísticas de verificação\n",
    "print(\"\\n[2.A] Estatísticas após Min-Max Scaling\")\n",
    "print(\n",
    "    pd.DataFrame(\n",
    "        {\n",
    "            \"min\": X_minmax.min(),\n",
    "            \"max\": X_minmax.max(),\n",
    "            \"mean\": X_minmax.mean(),\n",
    "            \"std\": X_minmax.std(ddof=0),\n",
    "        }\n",
    "    ).round(3)\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 3.B) Normalização via PowerTransform (Yeo-Johnson + padronização)\n",
    "# ---------------------------------------------------------------\n",
    "power = PowerTransformer(method=\"yeo-johnson\", standardize=True)\n",
    "X_power = pd.DataFrame(\n",
    "    power.fit_transform(X.values),\n",
    "    columns=data.feature_names,\n",
    ")\n",
    "\n",
    "# Exibir estatísticas de verificação\n",
    "print(\"\\n[3.B] Estatísticas após PowerTransform (Yeo-Johnson)\")\n",
    "print(\n",
    "    pd.DataFrame(\n",
    "        {\n",
    "            \"mean\": X_power.mean(),\n",
    "            \"std\": X_power.std(ddof=0),\n",
    "            \"skewness\": X_power.skew(),\n",
    "        }\n",
    "    ).round(3)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33e14a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[4(a)] Estatísticas após Min-Max Scaling (0–1):\n",
      "                         min  max   mean    std\n",
      "mean radius              0.0  1.0  0.338  0.167\n",
      "mean texture             0.0  1.0  0.324  0.145\n",
      "mean perimeter           0.0  1.0  0.333  0.168\n",
      "mean area                0.0  1.0  0.217  0.149\n",
      "mean smoothness          0.0  1.0  0.395  0.127\n",
      "mean compactness         0.0  1.0  0.261  0.162\n",
      "mean concavity           0.0  1.0  0.208  0.187\n",
      "mean concave points      0.0  1.0  0.243  0.193\n",
      "mean symmetry            0.0  1.0  0.380  0.138\n",
      "mean fractal dimension   0.0  1.0  0.270  0.149\n",
      "radius error             0.0  1.0  0.106  0.100\n",
      "texture error            0.0  1.0  0.189  0.122\n",
      "perimeter error          0.0  1.0  0.099  0.095\n",
      "area error               0.0  1.0  0.063  0.085\n",
      "smoothness error         0.0  1.0  0.181  0.102\n",
      "compactness error        0.0  1.0  0.174  0.134\n",
      "concavity error          0.0  1.0  0.081  0.076\n",
      "concave points error     0.0  1.0  0.223  0.117\n",
      "symmetry error           0.0  1.0  0.178  0.116\n",
      "fractal dimension error  0.0  1.0  0.100  0.091\n",
      "worst radius             0.0  1.0  0.297  0.172\n",
      "worst texture            0.0  1.0  0.364  0.164\n",
      "worst perimeter          0.0  1.0  0.283  0.167\n",
      "worst area               0.0  1.0  0.171  0.140\n",
      "worst smoothness         0.0  1.0  0.404  0.151\n",
      "worst compactness        0.0  1.0  0.220  0.153\n",
      "worst concavity          0.0  1.0  0.217  0.166\n",
      "worst concave points     0.0  1.0  0.394  0.226\n",
      "worst symmetry           0.0  1.0  0.263  0.122\n",
      "worst fractal dimension  0.0  1.0  0.190  0.118\n",
      "\n",
      "[4(b)] Estatísticas após Standard Scaling:\n",
      "                         mean  std    min     max\n",
      "mean radius              -0.0  1.0 -2.030   3.971\n",
      "mean texture             -0.0  1.0 -2.229   4.652\n",
      "mean perimeter           -0.0  1.0 -1.985   3.976\n",
      "mean area                -0.0  1.0 -1.454   5.251\n",
      "mean smoothness           0.0  1.0 -3.112   4.771\n",
      "mean compactness         -0.0  1.0 -1.610   4.568\n",
      "mean concavity           -0.0  1.0 -1.115   4.244\n",
      "mean concave points       0.0  1.0 -1.262   3.928\n",
      "mean symmetry            -0.0  1.0 -2.744   4.485\n",
      "mean fractal dimension   -0.0  1.0 -1.820   4.911\n",
      "radius error             -0.0  1.0 -1.060   8.907\n",
      "texture error            -0.0  1.0 -1.554   6.655\n",
      "perimeter error           0.0  1.0 -1.044   9.462\n",
      "area error               -0.0  1.0 -0.738  11.042\n",
      "smoothness error         -0.0  1.0 -1.776   8.030\n",
      "compactness error        -0.0  1.0 -1.298   6.143\n",
      "concavity error           0.0  1.0 -1.058  12.073\n",
      "concave points error     -0.0  1.0 -1.913   6.650\n",
      "symmetry error           -0.0  1.0 -1.533   7.072\n",
      "fractal dimension error  -0.0  1.0 -1.097   9.852\n",
      "worst radius             -0.0  1.0 -1.727   4.094\n",
      "worst texture             0.0  1.0 -2.224   3.886\n",
      "worst perimeter          -0.0  1.0 -1.693   4.287\n",
      "worst area                0.0  1.0 -1.222   5.930\n",
      "worst smoothness         -0.0  1.0 -2.683   3.955\n",
      "worst compactness        -0.0  1.0 -1.444   5.113\n",
      "worst concavity           0.0  1.0 -1.306   4.701\n",
      "worst concave points     -0.0  1.0 -1.745   2.686\n",
      "worst symmetry           -0.0  1.0 -2.161   6.046\n",
      "worst fractal dimension   0.0  1.0 -1.602   6.847\n",
      "\n",
      "[4(c)] Verificação da norma L2 de cada amostra (deve ser 1.0):\n",
      "count    569.0\n",
      "mean       1.0\n",
      "std        0.0\n",
      "min        1.0\n",
      "25%        1.0\n",
      "50%        1.0\n",
      "75%        1.0\n",
      "max        1.0\n",
      "dtype: float64\n",
      "Norma L2 mínima, média, máxima: 0.9999999999999998 1.0 1.0000000000000002\n",
      "Média das primeiras 5 features após transformação: [-0.02192476 -0.01770765 -0.02353115 -0.02735775 -0.01355427]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nSim. As transformações podem ser encadeadas em um Pipeline do scikit-learn:\\n\\nfrom sklearn.pipeline import make_pipeline\\npipe = make_pipeline(\\n    StandardScaler(),   # passo 1\\n    Normalizer(norm=\"l2\")  # passo 2\\n)\\nX_transf = pipe.fit_transform(X)\\n\\nA ordem importa: normalmente usa-se um escalonamento (MinMax ou Standard)\\nseguido pela normalização L2 se o modelo exigir vetores de unidade (ex.\\nk-NN baseado em similaridade de cosseno). A aplicação simultânea deve\\nser justificada pelo algoritmo ou pelo pré-processamento desejado.\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ---------------------------------------------------------------\n",
    "# Questão 4 — Breast Cancer Dataset: Normalização, Escalonamento \n",
    "#             e Regularização\n",
    "# ---------------------------------------------------------------\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 4(a) Min-Max Scaling (0 – 1)\n",
    "# ---------------------------------------------------------------\n",
    "minmax = MinMaxScaler()\n",
    "X_minmax = pd.DataFrame(minmax.fit_transform(X), columns=data.feature_names)\n",
    "\n",
    "print(\"\\n[4(a)] Estatísticas após Min-Max Scaling (0–1):\")\n",
    "print(\n",
    "    pd.DataFrame(\n",
    "        {\n",
    "            \"min\": X_minmax.min(),\n",
    "            \"max\": X_minmax.max(),\n",
    "            \"mean\": X_minmax.mean(),\n",
    "            \"std\": X_minmax.std(ddof=0),\n",
    "        }\n",
    "    ).round(3)\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 4(b) Standard Scaling (média 0, desvio-padrão 1)\n",
    "# ---------------------------------------------------------------\n",
    "std_scaler = StandardScaler()\n",
    "X_std = pd.DataFrame(std_scaler.fit_transform(X), columns=data.feature_names)\n",
    "\n",
    "print(\"\\n[4(b)] Estatísticas após Standard Scaling:\")\n",
    "print(\n",
    "    pd.DataFrame(\n",
    "        {\n",
    "            \"mean\": X_std.mean(),\n",
    "            \"std\": X_std.std(ddof=0),\n",
    "            \"min\": X_std.min(),\n",
    "            \"max\": X_std.max(),\n",
    "        }\n",
    "    ).round(3)\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 4(c) Regularização L2 (normalização por vetor linha)\n",
    "# ---------------------------------------------------------------\n",
    "# Normalizer aplica normalização L2 em cada amostra (linha), \n",
    "# resultando em vetores de norma 1.\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "l2_normalizer = Normalizer(norm=\"l2\")\n",
    "X_l2 = pd.DataFrame(l2_normalizer.fit_transform(X), columns=data.feature_names)\n",
    "\n",
    "# Conferir se a norma de cada vetor linha é 1\n",
    "normas = np.linalg.norm(X_l2.values, ord=2, axis=1)\n",
    "print(\"\\n[4(c)] Verificação da norma L2 de cada amostra (deve ser 1.0):\")\n",
    "print(pd.Series(normas).describe().round(3))\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 4(d) Aplicação simultânea de transformações\n",
    "# ---------------------------------------------------------------\n",
    "pipe = make_pipeline(\n",
    "    StandardScaler(),      # Escalonamento padrão\n",
    "    Normalizer(norm=\"l2\")  # Regularização L2 por amostra\n",
    ")\n",
    "\n",
    "# Ajusta ao conjunto de dados e transforma\n",
    "X_transf = pipe.fit_transform(X)\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# Comprovações rápidas\n",
    "# ---------------------------------------------------------------\n",
    "# 1. Norma L2 de cada amostra deve ser 1\n",
    "normas = np.linalg.norm(X_transf, ord=2, axis=1)\n",
    "print(\"Norma L2 mínima, média, máxima:\", normas.min(), normas.mean(), normas.max())\n",
    "\n",
    "# 2. As features foram padronizadas antes da normalização\n",
    "#    (as médias após o passo L2 tendem a zero, mas não exatamente,\n",
    "#     pois a normalização altera as escalas relativas)\n",
    "print(\"Média das primeiras 5 features após transformação:\",\n",
    "      np.mean(X_transf, axis=0)[:5])\n",
    "#\n",
    "#Sim. As transformações podem ser encadeadas em um Pipeline do scikit-learn:\n",
    "#from sklearn.pipeline import make_pipeline\n",
    "#pipe = make_pipeline(\n",
    "#    StandardScaler(),   # passo 1\n",
    "#    Normalizer(norm=\"l2\")  # passo 2\n",
    "#)\n",
    "#X_transf = pipe.fit_transform(X)\n",
    "#A ordem importa: normalmente usa-se um escalonamento (MinMax ou Standard)\n",
    "#seguido pela normalização L2 se o modelo exigir vetores de unidade (ex.\n",
    "#k-NN baseado em similaridade de cosseno). A aplicação simultânea deve\n",
    "#ser justificada pelo algoritmo ou pelo pré-processamento desejado.\n",
    "#\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
